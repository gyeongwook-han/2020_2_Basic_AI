9. 소감
제대로 하고 있는 것인지 의문이 들만큼 정말 긴 시간이었다.

나는 이 프로젝트에게 '루시'라는 이름을 지어주고싶다.
왜냐하면 학습되는 것을 기다리며 영화 '루시'가 생각났기 때문이다.

이 모델의 학습은 생각했던것보다 훨씬 긴 시간동안 진행되었는데
그 과정을 바라보며 영화 '루시'의 주인공처럼 완벽하게 뉴런들을 다룰 수 있게 되어서
진화를 하고 있는게 아닐까 하는 상상까지 했기 때문이다.

너무 오래걸린다고 생각하며 얼마나 많은 숫자가 사용될지 유닛의 갯수같은 핵심 숫자들만 곱해봤는데
784 * 256 * 128 * 10 * 60000(테스트 셋) * 400(에포크)
결과는 6,165,626,880,000,000 으로 한눈에 읽지도 못할만큼 엄청난 데이터였다.

10. 배운점 및 다음 시도에 수정할 사항
수정해볼 사항
(1). 활성화 함수
(2). Layer 수
(3). 만들어진 parameters(pkl data)를 npz파일로 변환하여 활용하기
(4). 학습을 중간에 중단하고 그 때까지 세이브 된 데이터를 로드해서 마저 학습시키기

그 외 중요한 점
(1). 타임 로그
(2). 더 많은 중간 점검 코드
(3). 주중 시간대(야간이 아닌 시간): 너무 힘이 듭니다.

위에 작성한 항목들은 이 실습 프로젝트를 통해 중요하다고 느껴진 것들이다.
강의 중 교수님께서 말씀해주신 기법과 관련된 것들이 꽤 있는데 왜 그 기술들이 대단한 것인지 배울 수 있었다.

첫째로 GPU의 발전이 어째서 ai의 발전을 초래했는지 확실하게 느낄 수 있었다.
교수님께서 5분 걸리셨다는 일이 나의 환경에서는 11시반에 시작해서 3시반에 끝났다.
4시간(240분), 즉 GPU가속을 사용하지 못하는 나의 환경은 GPU가 발전한 이후인 교수님 환경보다 48배나 더 긴 시간이 걸린 것이다.
단 2개의 hidden layer를 포함한 모델이 이렇게나 오래걸리는데 
여러 환경적, 요소적 차이 때문에 정확하게 계산할 수는 없지만 'DL' 분야에서 GPU의 가속이 얼마나 중요하게 작용하는지는 짐작할 수 있었다.

다음은 활성화 함수에 대한 것이다.
생각보다 너무 오래걸린다고 느낀 이후로 학습이 끝날 때 까지 처음부터 활성화함수 ReLU를 사용하지 않은 것을 후회했다.
이것은 다음 시도 때 확실히 속도의 차이를 체감할 수 있을 것이라고 생각된다.
교수님께서 더 빠를 것이라고 말씀해주셨던 ReLU, LeackReLU,, 기다리는 내내 머릿속에서 쉬지않고 맴돌았다.
현재 이 실습에서는 시그모이드(sigmoid)를 사용했으니 다른 함수를 사용해보면 재미있을 것 같다.

활성화 함수에서 차이를 확인할 수 있었듯이 layer 갯수의 변화가 시간과 정확도 면에서 얼마나 큰 차이를 내는지 확인해보면 재밌을 것 같다.

마지막으로 타임 로그를 찍지 않은 것을 후회했다.
중간 점검 코드의 빈도가 너무 길다고 느껴졌고 (이 프로젝트 기준 50에포크에 한번)
다른 프로젝트를 진행하게 된다면 task가 많은 프로젝트 일수록 시간이 오래 걸릴 것이기 때문에
헛수고를 예방하는 차원에서 타임 로그와 디테일한 점검 코드를 작성하는 것은 그 자체로 중요할 듯하다.

시간이 늦어 이런저런 테스트를 바로 해볼 수 없어서 상당히 아쉬웠다.
pkl data를 세이브 했으나 혹시나 잘못될까 하는 마음에 중단할 수 없었다.
이미 한번의 성공한 이력이 있으니 다음번에는 의도적으로 학습을 중단시켜보며 세이브 & 로드하는 법을 잘 배워야겠다.


